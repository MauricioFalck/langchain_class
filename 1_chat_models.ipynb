{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Masterclass\n",
    "\n",
    "## Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup\n",
    "\n",
    "Install the libraries required, and comment if this step is already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the APIs require an environment variable with the API key, in this case we add the API key to a .env file and load it using the *dotenv* library. The load_dotenv function loads a .env file and set its content as OS environment variables. \n",
    "\n",
    "For example, the content of the .env file could be:\n",
    "\n",
    "        OPENAPI_API_KEY=\"The api key\"\n",
    "\n",
    "*Note*: This step is not required for Ollama in this scenario. Since it won't find a .env file, the output will be False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "# returns True if .env file was loaded\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat model is the instance of the LLM that we are using to chat. Langchain provides many different options of providers that we can use depending on the requirements, full list [here](https://python.langchain.com/docs/integrations/platforms/). In our case, we will use Ollama.\n",
    "\n",
    "We will import the **ChatOllama** library from *langchain_ollama*, which is an implementation of a chat LLM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the ChatOllama with the model that we want to use. In this case, I will use Llama 3.2 that has been previously installed in Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interate with the model we use the **invoke** method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full result:\n",
      "content='To find the length of the other side in a right-angled triangle, we can use the Pythagorean theorem:\\n\\na² + b² = c²\\n\\nwhere:\\n- a is the length of one side (in this case, 3)\\n- b is the length of the other side (which we want to find)\\n- c is the length of the hypotenuse (5)\\n\\nWe can plug in the known values and solve for b:\\n\\n3² + b² = 5²\\n9 + b² = 25\\n\\nSubtracting 9 from both sides gives us:\\n\\nb² = 16\\n\\nTaking the square root of both sides (and considering that the length cannot be negative) gives us:\\n\\nb = √16 = 4\\n\\nSo, the length of the other side is 4 units.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2024-11-11T19:15:53.145441535Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2154357766, 'load_duration': 11418670, 'prompt_eval_count': 56, 'prompt_eval_duration': 50000000, 'eval_count': 169, 'eval_duration': 2092000000} id='run-1c3e40b3-543b-41d2-9375-d7a110cf44b0-0' usage_metadata={'input_tokens': 56, 'output_tokens': 169, 'total_tokens': 225}\n",
      "Content only:\n",
      "To find the length of the other side in a right-angled triangle, we can use the Pythagorean theorem:\n",
      "\n",
      "a² + b² = c²\n",
      "\n",
      "where:\n",
      "- a is the length of one side (in this case, 3)\n",
      "- b is the length of the other side (which we want to find)\n",
      "- c is the length of the hypotenuse (5)\n",
      "\n",
      "We can plug in the known values and solve for b:\n",
      "\n",
      "3² + b² = 5²\n",
      "9 + b² = 25\n",
      "\n",
      "Subtracting 9 from both sides gives us:\n",
      "\n",
      "b² = 16\n",
      "\n",
      "Taking the square root of both sides (and considering that the length cannot be negative) gives us:\n",
      "\n",
      "b = √16 = 4\n",
      "\n",
      "So, the length of the other side is 4 units.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model with a message\n",
    "result = model.invoke(\n",
    "    input=\"Assuming a right-angled triangle where one side is 3, the hypotenuse is 5, what is the size of the other side?\"\n",
    ")\n",
    "print(\"Full result:\")\n",
    "print(result)\n",
    "print(\"Content only:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SystemMessage**: Message for priming AI behavior, usually passed in as the first of a sequence of input messages.\n",
    "- **HumanMessage**: Message from a human to the AI model.\n",
    "- **AIMessage**: Message from the AI model as a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(\n",
    "    content=\"You are a math teacher assistant. Describe the solution step by step.\"\n",
    ")\n",
    "human_message = HumanMessage(\n",
    "    content=\"Assuming a right-angled triangle where one side is 3, the hypotenuse is 5, what is the size of the other side?\"\n",
    ")\n",
    "messages = [system_message, human_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from AI: To solve this problem, we can use the Pythagorean theorem. The Pythagorean theorem states that in a right-angled triangle, the square of the length of the hypotenuse (c) is equal to the sum of the squares of the lengths of the other two sides (a and b). Mathematically, it's expressed as:\n",
      "\n",
      "c² = a² + b²\n",
      "\n",
      "We are given one side (let's call it 'a') which is 3 units long. We need to find the length of the other side (let's call it 'b'). The hypotenuse (c) is 5 units long.\n",
      "\n",
      "Substitute the values into the equation:\n",
      "\n",
      "c² = a² + b²\n",
      "5² = 3² + b²\n",
      "\n",
      "First, calculate the squares:\n",
      "25 = 9 + b²\n",
      "\n",
      "Now, isolate b² by subtracting 9 from both sides of the equation:\n",
      "b² = 16\n",
      "\n",
      "To find the length of side 'b', take the square root of both sides:\n",
      "b = √16\n",
      "b = 4\n",
      "\n",
      "Therefore, the size of the other side is 4 units.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model with messages\n",
    "result = model.invoke(input=messages)\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Chatbot\n",
    "\n",
    "The LLM model has no history of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hi, my name is John.\n",
      "Answer from AI: Hello John! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Question: What is my name?\n",
      "Answer from AI: I don't have any information about your name. We just started our conversation, and I don't retain any personal data about individuals. If you'd like to share your name with me, I'm happy to chat with you!\n"
     ]
    }
   ],
   "source": [
    "human_message_1 = HumanMessage(content=\"Hi, my name is John.\")\n",
    "human_message_2 = HumanMessage(content=\"What is my name?\")\n",
    "print(f\"Question: {human_message_1.content}\")\n",
    "result = model.invoke(input=[human_message_1])\n",
    "print(f\"Answer from AI: {result.content}\")\n",
    "print(f\"Question: {human_message_2.content}\")\n",
    "result = model.invoke(input=[human_message_2])\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a chat with a model that has a history of the conversation, we invoke the model sending all the interactions as the content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hi, my name is John.\n",
      "Answer from AI: Hello John! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit?\n",
      "Question: What is my name?\n",
      "Answer from AI: Your name is John. You mentioned it earlier when we started our conversation.\n"
     ]
    }
   ],
   "source": [
    "# make the first question\n",
    "chat_history = [human_message_1]  # Add first user message to the chat history\n",
    "\n",
    "print(\n",
    "    f\"Question: {chat_history[-1].content}\"\n",
    ")  # selects the content of the last item in the chat_history\n",
    "result = model.invoke(input=chat_history)\n",
    "print(f\"Answer from AI: {result.content}\")\n",
    "\n",
    "chat_history.append(\n",
    "    AIMessage(content=result.content)\n",
    ")  # Add AI message to the chat history\n",
    "\n",
    "# make the second question\n",
    "chat_history.append(human_message_2)  # Add second user message to the chat history\n",
    "\n",
    "print(f\"Question: {chat_history[-1].content}\")\n",
    "result = model.invoke(input=chat_history)\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a simple chatbot with the model that keeps the context of the conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hi John! I'm happy to help you with your math questions or problems. What kind of math are you working on? Do you have a specific problem you'd like to solve or need help understanding a concept?\n",
      "AI: Your name is John. We had this conversation at the start, and I remember that we established it as your name. How can I assist you today, John? Do you have any math-related questions or topics you'd like to discuss?\n",
      "AI: It seems like we're repeating ourselves, John! You've mentioned your name a few times already. But that's okay, I'm happy to chat with you again if you need help with anything math-related or not. What's on your mind today?\n",
      "AI: John, it's been established earlier in our conversation that your name is indeed John. We don't have any new information about your name, so I'm going to take a guess that you're just testing me or seeing if I remember correctly! Either way, the answer is still... John!\n"
     ]
    }
   ],
   "source": [
    "chat_history = [system_message]  # Add system message to chat history\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    query = input(prompt=\"You: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    chat_history.append(HumanMessage(content=query))  # Add user message\n",
    "\n",
    "    # Get AI response using history\n",
    "    result = model.invoke(input=chat_history)\n",
    "    response = result.content\n",
    "    chat_history.append(AIMessage(content=response))  # Add AI message\n",
    "\n",
    "    print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use persistent storage (MongoDB)\n",
    "\n",
    "First, we load the library and create a connection to the MongoDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb import MongoDBChatMessageHistory\n",
    "\n",
    "history = MongoDBChatMessageHistory(\n",
    "    session_id=\"chat_history_session\",\n",
    "    connection_string=\"mongodb://mongoadmin:secret@localhost:27017/\",\n",
    "    database_name=\"chat_db\",\n",
    "    collection_name=\"chat_histories\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just as we did in the previous example, we add the messages to the MongoDB storage and use them as a context to the conversation using the *messages* property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear any previous data from collection\n",
    "history.clear()\n",
    "\n",
    "while True:\n",
    "    query: str = input(prompt=\"You: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    history.add_message(message=HumanMessage(content=query))  # Add user message\n",
    "    print(history.messages)\n",
    "\n",
    "    result = model.invoke(input=history.messages)\n",
    "    response = result.content\n",
    "    history.add_message(message=AIMessage(content=response))  # Add AI message\n",
    "    print(f\"AI: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
